{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append(r'D:\\Programming\\3rd_party\\keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from imp import reload\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data\n",
    "load_data = reload(load_data)\n",
    "import keras_unet_divrikwicky_model\n",
    "keras_unet_divrikwicky_model = reload(keras_unet_divrikwicky_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_MODE_RANGE = 0 # off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data.LoadData(train_data = True, DEV_MODE_RANGE = DEV_MODE_RANGE, to_gray = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fold_no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images, train_masks, validate_images, validate_masks = load_data.SplitTrainData(train_df, test_fold_no)\n",
    "train_images.shape, train_masks.shape, validate_images.shape, validate_masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducability setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rn\n",
    "\n",
    "kSeed = 241075\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "np.random.seed(kSeed)\n",
    "rn.seed(kSeed)\n",
    "\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(kSeed)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IOU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.array([0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95])\n",
    "\n",
    "def iou(img_true, img_pred):\n",
    "    assert (img_true.shape[-1]==1) and (len(img_true.shape)==3) or (img_true.shape[-1]!=1) and (len(img_true.shape)==2)\n",
    "    i = np.sum((img_true*img_pred) >0)\n",
    "    u = np.sum((img_true + img_pred) >0)\n",
    "    if u == 0:\n",
    "        return u\n",
    "    return i/u\n",
    "\n",
    "def iou_metric(img_true, img_pred):\n",
    "    img_pred = img_pred > 0.5 # added by sgx 20180728\n",
    "    if img_true.sum() == img_pred.sum() == 0:\n",
    "        scores = 1\n",
    "    else:\n",
    "        scores = (thresholds <= iou(img_true, img_pred)).mean()\n",
    "    return scores\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = len(y_true_in)\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    #print(\"metric = \",metric)\n",
    "    return np.mean(metric)\n",
    "\n",
    "# adapter for Keras\n",
    "def my_iou_metric(label, pred):\n",
    "    metric_value = tf.py_func(iou_metric_batch, [label, pred], tf.float64)\n",
    "    return metric_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val = np.mean(train_images.apply(np.mean))\n",
    "mean_std = np.mean(train_images.apply(np.std))\n",
    "mean_val, mean_std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '../3rd_party/albumentations')\n",
    "sys.path.insert(1, '../3rd_party/imgaug')\n",
    "import albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_image_size = 101\n",
    "nn_image_size = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_aug(p=1.):\n",
    "    return albumentations.Compose([\n",
    "        albumentations.Resize(augmented_image_size, augmented_image_size),\n",
    "        albumentations.HorizontalFlip(),\n",
    "        albumentations.RandomCrop(nn_image_size, nn_image_size),\n",
    "        albumentations.Normalize(mean = mean_val, std = mean_std, max_pixel_value = 1.0),\n",
    "    ], p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def more_aug(p=1.):\n",
    "    return albumentations.Compose([\n",
    "        albumentations.Resize(augmented_image_size, augmented_image_size),\n",
    "        albumentations.RandomScale(0.04),\n",
    "        albumentations.HorizontalFlip(),\n",
    "        albumentations.RandomCrop(nn_image_size, nn_image_size),\n",
    "        albumentations.Normalize(mean = mean_val, std = mean_std, max_pixel_value = 1.0),\n",
    "\n",
    "        albumentations.Blur(),\n",
    "        albumentations.Rotate(limit=5),\n",
    "        albumentations.RandomBrightness(),\n",
    "        albumentations.RandomContrast(),\n",
    "    ], p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "class AlbuDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, images, masks, batch_size=32, nn_image_size = 96, shuffle=True, aug_func = basic_aug):\n",
    "        'Initialization'\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.batch_size = batch_size\n",
    "        self.nn_image_size = nn_image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.images))\n",
    "        self.on_epoch_end()\n",
    "        self.augmentation = aug_func()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(len(self.images) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X, y = self.__data_generation(indexes)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.nn_image_size,self.nn_image_size, 3), dtype=np.float32)\n",
    "        y = np.empty((self.batch_size, self.nn_image_size,self.nn_image_size, 1), dtype=np.float32)\n",
    "\n",
    "        # Generate data\n",
    "        for i, index in enumerate(indexes):\n",
    "            image = self.images[index]\n",
    "            mask = None if self.masks is None else self.masks[index]\n",
    "            aug_res = self.augmentation(image = image, mask = mask)\n",
    "            image = aug_res['image']\n",
    "            X[i, ...] = image\n",
    "            mask = aug_res['mask']\n",
    "            y[i, ...] = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../3rd_party/segmentation_models')\n",
    "import segmentation_models\n",
    "segmentation_models = reload(segmentation_models)\n",
    "from segmentation_models.utils import set_trainable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'divrikwicky'\n",
    "backbone_name='inceptionv3'\n",
    "test_batch_size = 50\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "if model_name == 'FNN':\n",
    "    model = segmentation_models.FPN(backbone_name=backbone_name, input_shape=(None, None, channels), encoder_weights='imagenet', freeze_encoder=True)\n",
    "if model_name == 'Unet':\n",
    "    model = segmentation_models.Unet(backbone_name=backbone_name, input_shape=(None, None, channels), encoder_weights='imagenet', freeze_encoder=True)\n",
    "if model_name == 'divrikwicky':\n",
    "    model = keras_unet_divrikwicky_model.CreateModel(nn_image_size)\n",
    "    backbone_name = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_file = 'models_1/{model_name}_{backbone_name}_{test_fold_no}_x96.model'.format(model_name=model_name, backbone_name=backbone_name, test_fold_no=test_fold_no)\n",
    "model_out_file = 'models_2/{model_name}_{backbone_name}_{test_fold_no}.model'.format(model_name=model_name, backbone_name=backbone_name, test_fold_no=test_fold_no)\n",
    "model_out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model(model1_file, custom_objects={'my_iou_metric': my_iou_metric}) #, 'lavazs_loss': lavazs_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\", my_iou_metric]) #, my_iou_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_warmup = 2\n",
    "epochs = 250\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = AlbuDataGenerator(train_images, train_masks, batch_size=batch_size, nn_image_size = nn_image_size,\n",
    "                              shuffle=True, aug_func = more_aug)\n",
    "val_gen = AlbuDataGenerator(validate_images, validate_masks, batch_size=test_batch_size, nn_image_size = nn_image_size,\n",
    "                            shuffle=False, aug_func = basic_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../3rd_party/keras-tqdm')\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_my_iou_metric', mode = 'max',patience=50, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(model_out_file, monitor='val_my_iou_metric',\n",
    "                                   mode = 'max', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric', mode = 'max',factor=0.2, patience=20, min_lr=0.00001, verbose=1)\n",
    "\n",
    "'''\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath + '.hdf5', save_best_only=True)\n",
    "    csv_logger = CSVLogger(filepath+'_log.csv', separator=',', append=False)\n",
    "    return [es, msave, csv_logger]\n",
    "'''\n",
    "    \n",
    "\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                    validation_data=val_gen, \n",
    "                    epochs=epochs_warmup,\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr, TQDMNotebookCallback(leave_inner=True),\n",
    "                              CSVLogger(model_out_file+'_log.csv', separator=',', append=False)],\n",
    "                    validation_steps=len(val_gen)*3,\n",
    "                    workers=1,\n",
    "                    use_multiprocessing=False,\n",
    "                    verbose=0)\n",
    "\n",
    "set_trainable(model)\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                    validation_data=val_gen, \n",
    "                    epochs=epochs,\n",
    "                    initial_epoch = epochs_warmup,\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr, TQDMNotebookCallback(leave_inner=True),\n",
    "                              CSVLogger(model_out_file+'_log.csv', separator=',', append=True)],\n",
    "                    validation_steps=len(val_gen)*3,\n",
    "                    workers=1,\n",
    "                    use_multiprocessing=False,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
