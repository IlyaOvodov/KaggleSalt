{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "7d7bc35bfdb4aa0dbb83ad76872ffb50446c5295"
   },
   "outputs": [],
   "source": [
    "directory = '../../../../Kaggle_Data/Salt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'D:\\Programming\\Kaggle\\Salt\\pytorch-summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "import torchsummary \n",
    "\n",
    "\n",
    "def conv3x3(in_, out):\n",
    "    return nn.Conv2d(in_, out, 3, padding=1)\n",
    "\n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_, out):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            ConvRelu(in_channels, middle_channels),\n",
    "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNet11(nn.Module):\n",
    "    def __init__(self, num_filters=32):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Convolutions are from VGG11\n",
    "        self.encoder = models.vgg11().features\n",
    "        \n",
    "        # \"relu\" layer is taken from VGG probably for generality, but it's not clear \n",
    "        self.relu = self.encoder[1]\n",
    "        \n",
    "        self.conv1 = self.encoder[0]\n",
    "        self.conv2 = self.encoder[3]\n",
    "        self.conv3s = self.encoder[6]\n",
    "        self.conv3 = self.encoder[8]\n",
    "        self.conv4s = self.encoder[11]\n",
    "        self.conv4 = self.encoder[13]\n",
    "        self.conv5s = self.encoder[16]\n",
    "        self.conv5 = self.encoder[18]\n",
    "\n",
    "        self.center = DecoderBlock(num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec5 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec4 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4)\n",
    "        self.dec3 = DecoderBlock(num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2)\n",
    "        self.dec2 = DecoderBlock(num_filters * (4 + 2), num_filters * 2 * 2, num_filters)\n",
    "        self.dec1 = ConvRelu(num_filters * (2 + 1), num_filters)\n",
    "        \n",
    "        self.final = nn.Conv2d(num_filters, 1, kernel_size=1, )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.relu(self.conv1(x))\n",
    "        conv2 = self.relu(self.conv2(self.pool(conv1)))\n",
    "        conv3s = self.relu(self.conv3s(self.pool(conv2)))\n",
    "        conv3 = self.relu(self.conv3(conv3s))\n",
    "        conv4s = self.relu(self.conv4s(self.pool(conv3)))\n",
    "        conv4 = self.relu(self.conv4(conv4s))\n",
    "        conv5s = self.relu(self.conv5s(self.pool(conv4)))\n",
    "        conv5 = self.relu(self.conv5(conv5s))\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        # Deconvolutions with copies of VGG11 layers of corresponding size \n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "        return F.sigmoid(self.final(dec1))\n",
    "\n",
    "\n",
    "def unet11(**kwargs):\n",
    "    model = UNet11(**kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model():\n",
    "    model = unet11()\n",
    "    model.train()\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7114b9f3da03d4688ecfdecd7c7008a0be0c8004"
   },
   "outputs": [],
   "source": [
    "def load_image(path, mask = False):\n",
    "    \"\"\"\n",
    "    Load image from a given path and pad it on the sides, so that eash side is divisible by 32 (newtwork requirement)\n",
    "    \n",
    "    if pad = True:\n",
    "        returns image as numpy.array, tuple with padding in pixels as(x_min_pad, y_min_pad, x_max_pad, y_max_pad)\n",
    "    else:\n",
    "        returns image as numpy.array\n",
    "    \"\"\"\n",
    "    img = cv2.imread(str(path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    # Padding in needed for UNet models because they need image size to be divisible by 32 \n",
    "    if height % 32 == 0:\n",
    "        y_min_pad = 0\n",
    "        y_max_pad = 0\n",
    "    else:\n",
    "        y_pad = 32 - height % 32\n",
    "        y_min_pad = int(y_pad / 2)\n",
    "        y_max_pad = y_pad - y_min_pad\n",
    "        \n",
    "    if width % 32 == 0:\n",
    "        x_min_pad = 0\n",
    "        x_max_pad = 0\n",
    "    else:\n",
    "        x_pad = 32 - width % 32\n",
    "        x_min_pad = int(x_pad / 2)\n",
    "        x_max_pad = x_pad - x_min_pad\n",
    "    \n",
    "    img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad, x_min_pad, x_max_pad, cv2.BORDER_REFLECT_101)\n",
    "    if mask:\n",
    "        # Convert mask to 0 and 1 format\n",
    "        img = img[:, :, 0:1] // 255\n",
    "        return torch.from_numpy(img).float().permute([2, 0, 1])\n",
    "    else:\n",
    "        img = img / 255.0\n",
    "        return torch.from_numpy(img).float().permute([2, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "87e0c6c34c6916e43b8f4e8e1f6eb708f8049b3d"
   },
   "outputs": [],
   "source": [
    "# Adapted from vizualization kernel\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "class TGSSaltDataset(data.Dataset):\n",
    "    def __init__(self, root_path, file_list, is_test = False):\n",
    "        self.is_test = is_test\n",
    "        self.root_path = root_path\n",
    "        self.file_list = file_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index not in range(0, len(self.file_list)):\n",
    "            return self.__getitem__(np.random.randint(0, self.__len__()))\n",
    "        \n",
    "        file_id = self.file_list[index]\n",
    "        \n",
    "        image_folder = os.path.join(self.root_path, \"images\")\n",
    "        image_path = os.path.join(image_folder, file_id + \".png\")\n",
    "        \n",
    "        mask_folder = os.path.join(self.root_path, \"masks\")\n",
    "        mask_path = os.path.join(mask_folder, file_id + \".png\")\n",
    "        \n",
    "        image = load_image(image_path)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return (image,)\n",
    "        else:\n",
    "            mask = load_image(mask_path, mask = True)\n",
    "            return image, mask\n",
    "\n",
    "depths_df = pd.read_csv(os.path.join(directory, 'train.csv'))\n",
    "\n",
    "train_path = os.path.join(directory, 'train')\n",
    "file_list = list(depths_df['id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "89289aeceba7a47c7478e9a7fb1232cedeed70b2"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet11(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): ReLU(inplace)\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (relu): ReLU(inplace)\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3s): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4s): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5s): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (center): DecoderBlock(\n",
      "    (block): Sequential(\n",
      "      (0): ConvRelu(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU(inplace)\n",
      "      )\n",
      "      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (dec5): DecoderBlock(\n",
      "    (block): Sequential(\n",
      "      (0): ConvRelu(\n",
      "        (conv): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU(inplace)\n",
      "      )\n",
      "      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (dec4): DecoderBlock(\n",
      "    (block): Sequential(\n",
      "      (0): ConvRelu(\n",
      "        (conv): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU(inplace)\n",
      "      )\n",
      "      (1): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (dec3): DecoderBlock(\n",
      "    (block): Sequential(\n",
      "      (0): ConvRelu(\n",
      "        (conv): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU(inplace)\n",
      "      )\n",
      "      (1): ConvTranspose2d(256, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (dec2): DecoderBlock(\n",
      "    (block): Sequential(\n",
      "      (0): ConvRelu(\n",
      "        (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU(inplace)\n",
      "      )\n",
      "      (1): ConvTranspose2d(128, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (dec1): ConvRelu(\n",
      "    (conv): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation): ReLU(inplace)\n",
      "  )\n",
      "  (final): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package torchvision.models in torchvision:\n",
      "\n",
      "NAME\n",
      "    torchvision.models\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    alexnet\n",
      "    densenet\n",
      "    inception\n",
      "    resnet\n",
      "    squeezenet\n",
      "    vgg\n",
      "\n",
      "FILE\n",
      "    c:\\miniconda3\\lib\\site-packages\\torchvision\\models\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 96, 96]           1,792\n",
      "            Conv2d-2           [-1, 64, 96, 96]           1,792\n",
      "              ReLU-3           [-1, 64, 96, 96]               0\n",
      "              ReLU-4           [-1, 64, 96, 96]               0\n",
      "         MaxPool2d-5           [-1, 64, 48, 48]               0\n",
      "            Conv2d-6          [-1, 128, 48, 48]          73,856\n",
      "            Conv2d-7          [-1, 128, 48, 48]          73,856\n",
      "              ReLU-8          [-1, 128, 48, 48]               0\n",
      "              ReLU-9          [-1, 128, 48, 48]               0\n",
      "        MaxPool2d-10          [-1, 128, 24, 24]               0\n",
      "           Conv2d-11          [-1, 256, 24, 24]         295,168\n",
      "           Conv2d-12          [-1, 256, 24, 24]         295,168\n",
      "             ReLU-13          [-1, 256, 24, 24]               0\n",
      "             ReLU-14          [-1, 256, 24, 24]               0\n",
      "           Conv2d-15          [-1, 256, 24, 24]         590,080\n",
      "           Conv2d-16          [-1, 256, 24, 24]         590,080\n",
      "             ReLU-17          [-1, 256, 24, 24]               0\n",
      "             ReLU-18          [-1, 256, 24, 24]               0\n",
      "        MaxPool2d-19          [-1, 256, 12, 12]               0\n",
      "           Conv2d-20          [-1, 512, 12, 12]       1,180,160\n",
      "           Conv2d-21          [-1, 512, 12, 12]       1,180,160\n",
      "             ReLU-22          [-1, 512, 12, 12]               0\n",
      "             ReLU-23          [-1, 512, 12, 12]               0\n",
      "           Conv2d-24          [-1, 512, 12, 12]       2,359,808\n",
      "           Conv2d-25          [-1, 512, 12, 12]       2,359,808\n",
      "             ReLU-26          [-1, 512, 12, 12]               0\n",
      "             ReLU-27          [-1, 512, 12, 12]               0\n",
      "        MaxPool2d-28            [-1, 512, 6, 6]               0\n",
      "           Conv2d-29            [-1, 512, 6, 6]       2,359,808\n",
      "           Conv2d-30            [-1, 512, 6, 6]       2,359,808\n",
      "             ReLU-31            [-1, 512, 6, 6]               0\n",
      "             ReLU-32            [-1, 512, 6, 6]               0\n",
      "           Conv2d-33            [-1, 512, 6, 6]       2,359,808\n",
      "           Conv2d-34            [-1, 512, 6, 6]       2,359,808\n",
      "             ReLU-35            [-1, 512, 6, 6]               0\n",
      "             ReLU-36            [-1, 512, 6, 6]               0\n",
      "        MaxPool2d-37            [-1, 512, 3, 3]               0\n",
      "           Conv2d-38            [-1, 512, 3, 3]       2,359,808\n",
      "             ReLU-39            [-1, 512, 3, 3]               0\n",
      "         ConvRelu-40            [-1, 512, 3, 3]               0\n",
      "  ConvTranspose2d-41            [-1, 256, 6, 6]       1,179,904\n",
      "             ReLU-42            [-1, 256, 6, 6]               0\n",
      "     DecoderBlock-43            [-1, 256, 6, 6]               0\n",
      "           Conv2d-44            [-1, 512, 6, 6]       3,539,456\n",
      "             ReLU-45            [-1, 512, 6, 6]               0\n",
      "         ConvRelu-46            [-1, 512, 6, 6]               0\n",
      "  ConvTranspose2d-47          [-1, 256, 12, 12]       1,179,904\n",
      "             ReLU-48          [-1, 256, 12, 12]               0\n",
      "     DecoderBlock-49          [-1, 256, 12, 12]               0\n",
      "           Conv2d-50          [-1, 512, 12, 12]       3,539,456\n",
      "             ReLU-51          [-1, 512, 12, 12]               0\n",
      "         ConvRelu-52          [-1, 512, 12, 12]               0\n",
      "  ConvTranspose2d-53          [-1, 128, 24, 24]         589,952\n",
      "             ReLU-54          [-1, 128, 24, 24]               0\n",
      "     DecoderBlock-55          [-1, 128, 24, 24]               0\n",
      "           Conv2d-56          [-1, 256, 24, 24]         884,992\n",
      "             ReLU-57          [-1, 256, 24, 24]               0\n",
      "         ConvRelu-58          [-1, 256, 24, 24]               0\n",
      "  ConvTranspose2d-59           [-1, 64, 48, 48]         147,520\n",
      "             ReLU-60           [-1, 64, 48, 48]               0\n",
      "     DecoderBlock-61           [-1, 64, 48, 48]               0\n",
      "           Conv2d-62          [-1, 128, 48, 48]         221,312\n",
      "             ReLU-63          [-1, 128, 48, 48]               0\n",
      "         ConvRelu-64          [-1, 128, 48, 48]               0\n",
      "  ConvTranspose2d-65           [-1, 32, 96, 96]          36,896\n",
      "             ReLU-66           [-1, 32, 96, 96]               0\n",
      "     DecoderBlock-67           [-1, 32, 96, 96]               0\n",
      "           Conv2d-68           [-1, 32, 96, 96]          27,680\n",
      "             ReLU-69           [-1, 32, 96, 96]               0\n",
      "         ConvRelu-70           [-1, 32, 96, 96]               0\n",
      "           Conv2d-71            [-1, 1, 96, 96]              33\n",
      "================================================================\n",
      "Total params: 32,147,873\n",
      "Trainable params: 32,147,873\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 75.80\n",
      "Params size (MB): 122.63\n",
      "Estimated Total Size (MB): 198.54\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(model, (3, 96, 96))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "767dae95adfbe7fefcb4cc3dd397ea8ba0e3f8d1",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-79f071d6d936>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfile_list_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfile_list_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_list\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_list_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTGSSaltDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_list_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file_list' is not defined"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "file_list_val = file_list[::10]\n",
    "file_list_train = [f for f in file_list if f not in file_list_val]\n",
    "dataset = TGSSaltDataset(train_path, file_list_train)\n",
    "dataset_val = TGSSaltDataset(train_path, file_list_val)\n",
    "\n",
    "model = get_model()\n",
    "#\n",
    "\n",
    "learning_rate = 1e-4\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for e in range(100):\n",
    "    train_loss = []\n",
    "    for image, mask in tqdm.tqdm(data.DataLoader(dataset, batch_size = 30, shuffle = True)):\n",
    "        image = image.type(torch.float).to(device)\n",
    "        y_pred = model(image)\n",
    "        loss = loss_fn(y_pred, mask.to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    val_loss = []\n",
    "    for image, mask in data.DataLoader(dataset_val, batch_size = 50, shuffle = False):\n",
    "        image = image.to(device)\n",
    "        y_pred = model(image)\n",
    "\n",
    "        loss = loss_fn(y_pred, mask.to(device))\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "    with open(r'D:\\Temp\\log.txt', 'w') as f:\n",
    "        print(\"Epoch: %d, Train: %.3f, Val: %.3f\" % (e, np.mean(train_loss), np.mean(val_loss)), file = f)        \n",
    "    print(\"Epoch: %d, Train: %.3f, Val: %.3f\" % (e, np.mean(train_loss), np.mean(val_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60392c6252c7f2628db1e4fef8ae69e29f7e753e"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "test_path = os.path.join(directory, 'test')\n",
    "test_file_list = glob.glob(os.path.join(test_path, 'images', '*.png'))\n",
    "test_file_list = [f.split('\\\\')[-1].split('.')[0] for f in test_file_list]\n",
    "test_file_list[:3], test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "971e75a32512f23aee3cbc629df64c8079940e91"
   },
   "outputs": [],
   "source": [
    "print(len(test_file_list))\n",
    "test_dataset = TGSSaltDataset(test_path, test_file_list, is_test = True)\n",
    "\n",
    "all_predictions = []\n",
    "for image in tqdm.tqdm(data.DataLoader(test_dataset, batch_size = 30)):\n",
    "    image = image[0].type(torch.float).to(device)\n",
    "    y_pred = model(image).cpu().detach().numpy()\n",
    "    all_predictions.append(y_pred)\n",
    "all_predictions_stacked = np.vstack(all_predictions)[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "95e82b2a7155377310f1d743dd8b077f99cba657"
   },
   "outputs": [],
   "source": [
    "height, width = 101, 101\n",
    "\n",
    "if height % 32 == 0:\n",
    "    y_min_pad = 0\n",
    "    y_max_pad = 0\n",
    "else:\n",
    "    y_pad = 32 - height % 32\n",
    "    y_min_pad = int(y_pad / 2)\n",
    "    y_max_pad = y_pad - y_min_pad\n",
    "\n",
    "if width % 32 == 0:\n",
    "    x_min_pad = 0\n",
    "    x_max_pad = 0\n",
    "else:\n",
    "    x_pad = 32 - width % 32\n",
    "    x_min_pad = int(x_pad / 2)\n",
    "    x_max_pad = x_pad - x_min_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0c02220ba7768ac904e2e27f449393e5182cac5"
   },
   "outputs": [],
   "source": [
    "all_predictions_stacked = all_predictions_stacked[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fc57c37629a96ac4c16dfbc18546278591716613"
   },
   "outputs": [],
   "source": [
    "all_predictions_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65d1b4f79719da90faa68bb73359a5189ea70bfd"
   },
   "outputs": [],
   "source": [
    "test_dataset = TGSSaltDataset(test_path, test_file_list, is_test = True)\n",
    "\n",
    "val_predictions = []\n",
    "val_masks = []\n",
    "for image, mask in tqdm.tqdm(data.DataLoader(dataset_val, batch_size = 30)):\n",
    "    image = image.type(torch.float).to(device)\n",
    "    y_pred = model(image).cpu().detach().numpy()\n",
    "    val_predictions.append(y_pred)\n",
    "    val_masks.append(mask)\n",
    "    \n",
    "val_predictions_stacked = np.vstack(val_predictions)[:, 0, :, :]\n",
    "\n",
    "val_masks_stacked = np.vstack(val_masks)[:, 0, :, :]\n",
    "val_predictions_stacked = val_predictions_stacked[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]\n",
    "\n",
    "val_masks_stacked = val_masks_stacked[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]\n",
    "val_masks_stacked.shape, val_predictions_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dbd7cbac108805a401a445947b4cfd95721e10ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "metric_by_threshold = []\n",
    "for threshold in np.linspace(0, 1, 11):\n",
    "    val_binary_prediction = (val_predictions_stacked > threshold).astype(int)\n",
    "    \n",
    "    iou_values = []\n",
    "    for y_mask, p_mask in zip(val_masks_stacked, val_binary_prediction):\n",
    "        iou = jaccard_similarity_score(y_mask.flatten(), p_mask.flatten())\n",
    "        iou_values.append(iou)\n",
    "    iou_values = np.array(iou_values)\n",
    "    \n",
    "    accuracies = [\n",
    "        np.mean(iou_values > iou_threshold)\n",
    "        for iou_threshold in np.linspace(0.5, 0.95, 10)\n",
    "    ]\n",
    "    print('Threshold: %.1f, Metric: %.3f' % (threshold, np.mean(accuracies)))\n",
    "    metric_by_threshold.append((np.mean(accuracies), threshold))\n",
    "    \n",
    "best_metric, best_threshold = max(metric_by_threshold)\n",
    "print(best_metric, best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69b3e6549ac9dac536eacd7d116d1942a61b0b50"
   },
   "outputs": [],
   "source": [
    "threshold = best_threshold\n",
    "binary_prediction = (all_predictions_stacked > threshold).astype(int)\n",
    "\n",
    "def rle_encoding(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b > prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "all_masks = []\n",
    "for p_mask in list(binary_prediction):\n",
    "    p_mask = rle_encoding(p_mask)\n",
    "    all_masks.append(' '.join(map(str, p_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2d05d0ed3c54619523a55683d6e1afc22dace1f"
   },
   "outputs": [],
   "source": [
    "submit = pd.DataFrame([test_file_list, all_masks]).T\n",
    "submit.columns = ['id', 'rle_mask']\n",
    "submit.to_csv('submit_baseline2.csv.gz', compression = 'gzip', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "811efa49c3fc86adbf40e13003664a8212f77fed"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 'r',\n",
       " 'b',\n",
       " 's',\n",
       " 'n',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " 'f',\n",
       " 'y',\n",
       " 'q',\n",
       " 'z',\n",
       " 'k',\n",
       " 'h',\n",
       " 'd',\n",
       " 'g',\n",
       " 'j',\n",
       " 'l',\n",
       " 'm',\n",
       " 'c',\n",
       " 'a',\n",
       " 'u',\n",
       " 'w',\n",
       " 'p',\n",
       " 'x',\n",
       " 'o']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "sorted(\"abcdefghijklmnopqrstuvwxyz\", key = lambda x: hash(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
